{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11396811,"sourceType":"datasetVersion","datasetId":7137671}],"dockerImageVersionId":31011,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport random\nimport torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom collections import Counter\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nimport nltk\nnltk.download('punkt')\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n\nclass Vocab:\n    def __init__(self, tokens, min_freq=1):\n        counter = Counter(tokens)\n        self.token_to_idx = {\n            '<pad>': 0,\n            '<sos>': 1,\n            '<eos>': 2,\n            '<unk>': 3\n        }\n        for token, freq in counter.items():\n            if freq >= min_freq and token not in self.token_to_idx:\n                self.token_to_idx[token] = len(self.token_to_idx)\n        self.idx_to_token = {idx: tok for tok, idx in self.token_to_idx.items()}\n\n    def __len__(self):\n        return len(self.token_to_idx)\n\n    def encode(self, tokens):\n        return [self.token_to_idx.get(tok, self.token_to_idx['<unk>']) for tok in tokens]\n\n    def decode(self, indices):\n        return [self.idx_to_token.get(idx, '<unk>') for idx in indices]\n\n\ndef preprocess_sentence(sentence):\n    sentence = sentence.lower().strip()\n    sentence = re.sub(r\"([?.!,¬ø])\", r\" \\1 \", sentence)\n    sentence = re.sub(r\"[^a-zA-Z?.!,¬ø]+\", r\" \", sentence)\n    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n    return word_tokenize(sentence)\n\n\ndef load_dataset(file_path, num_samples=None):\n    with open(file_path, encoding='utf-8') as f:\n        lines = f.read().strip().split('\\n')\n\n    if num_samples:\n        lines = lines[:num_samples]\n\n    input_sentences = []\n    target_sentences = []\n\n    for line in lines:\n        en, es = line.split('\\t')\n        en_tokens = ['<sos>'] + preprocess_sentence(en) + ['<eos>']\n        es_tokens = ['<sos>'] + preprocess_sentence(es) + ['<eos>']\n        input_sentences.append(en_tokens)\n        target_sentences.append(es_tokens)\n\n    return input_sentences, target_sentences\n\n\ndef build_dataset(file_path='spa.txt', num_samples=10000):\n    inputs, targets = load_dataset(file_path, num_samples)\n\n    input_vocab = Vocab([tok for sent in inputs for tok in sent])\n    target_vocab = Vocab([tok for sent in targets for tok in sent])\n\n    input_indices = [torch.tensor(input_vocab.encode(s)) for s in inputs]\n    target_indices = [torch.tensor(target_vocab.encode(s)) for s in targets]\n\n    input_padded = pad_sequence(input_indices, batch_first=True, padding_value=input_vocab.token_to_idx['<pad>'])\n    target_padded = pad_sequence(target_indices, batch_first=True, padding_value=target_vocab.token_to_idx['<pad>'])\n\n    total = len(input_padded)\n    train_end = int(0.8 * total)\n    val_end = int(0.9 * total)\n\n    inp_train = input_padded[:train_end]\n    targ_train = target_padded[:train_end]\n\n    inp_val = input_padded[train_end:val_end]\n    targ_val = target_padded[train_end:val_end]\n\n    inp_test = input_padded[val_end:]\n    targ_test = target_padded[val_end:]\n\n    return inp_train, inp_val, inp_test, targ_train, targ_val, targ_test, input_vocab, target_vocab","metadata":{"_uuid":"3ede6cac-ce0c-421c-8632-51399e39a60f","_cell_guid":"8efed5e6-4604-4b97-95be-aeb0ce49089b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:16.507574Z","iopub.execute_input":"2025-04-16T13:43:16.507791Z","iopub.status.idle":"2025-04-16T13:43:53.387150Z","shell.execute_reply.started":"2025-04-16T13:43:16.507763Z","shell.execute_reply":"2025-04-16T13:43:53.386427Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"[nltk_data] Error loading punkt: <urlopen error [Errno -3] Temporary\n[nltk_data]     failure in name resolution>\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hid_dim, num_layers, dropout):\n        super(Encoder, self).__init__()\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout=dropout, batch_first=True)\n\n    def forward(self, src):\n        embedded = self.embedding(src)\n        outputs, (hidden, cell) = self.lstm(embedded)\n        return hidden, cell\n\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hid_dim, num_layers, dropout):\n        super(Decoder, self).__init__()\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hid_dim, num_layers, dropout=dropout, batch_first=True)\n        self.fc_out = nn.Linear(hid_dim, output_dim)\n\n    def forward(self, input, hidden, cell):\n        input = input.unsqueeze(1)  # [batch_size, 1]\n        embedded = self.embedding(input)\n        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n        prediction = self.fc_out(output.squeeze(1))  # [batch_size, output_dim]\n        return prediction, hidden, cell\n\n\nclass Seq2Seq(nn.Module):\n    def __init__(self, encoder, decoder, device):\n        super(Seq2Seq, self).__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n        batch_size, trg_len = trg.shape\n        trg_vocab_size = self.decoder.fc_out.out_features\n        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n\n        hidden, cell = self.encoder(src)\n\n        input = trg[:, 0]  # <sos>\n\n        for t in range(1, trg_len):\n            output, hidden, cell = self.decoder(input, hidden, cell)\n            outputs[:, t] = output\n\n            top1 = output.argmax(1)\n            input = trg[:, t] if torch.rand(1).item() < teacher_forcing_ratio else top1\n\n        return outputs","metadata":{"_uuid":"f408bb74-17b3-4326-b729-04b4c84850f6","_cell_guid":"2663f727-0301-4381-bf2e-ea09c971f0c7","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.388620Z","iopub.execute_input":"2025-04-16T13:43:53.388957Z","iopub.status.idle":"2025-04-16T13:43:53.398669Z","shell.execute_reply.started":"2025-04-16T13:43:53.388938Z","shell.execute_reply":"2025-04-16T13:43:53.398013Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom nltk.translate.bleu_score import sentence_bleu\nfrom tqdm import tqdm\n\n\ndef train_model(model, dataloader, optimizer, criterion, clip, device):\n    model.train()\n    epoch_loss = 0\n\n    for src, trg in tqdm(dataloader, desc=\"Training\"):\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n\n        output = model(src, trg)  # [batch_size, trg_len, output_dim]\n        output_dim = output.shape[-1]\n\n        output = output[:, 1:].reshape(-1, output_dim)\n        trg = trg[:, 1:].reshape(-1)\n\n        loss = criterion(output, trg)\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(dataloader)","metadata":{"_uuid":"4a84be25-b56c-46a0-9f42-4c61dd443c1d","_cell_guid":"a06dd69e-c8ee-4b2b-9d13-b9900e87d7f8","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.399405Z","iopub.execute_input":"2025-04-16T13:43:53.399656Z","iopub.status.idle":"2025-04-16T13:43:53.468601Z","shell.execute_reply.started":"2025-04-16T13:43:53.399640Z","shell.execute_reply":"2025-04-16T13:43:53.467949Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def evaluate_model(model, dataloader, criterion, device):\n    model.eval()\n    total_loss = 0\n\n    with torch.no_grad():\n        for src, trg in dataloader:\n            src, trg = src.to(device), trg.to(device)\n            output = model(src, trg, teacher_forcing_ratio=0.0)\n\n            output_dim = output.shape[-1]\n            output = output[:, 1:].reshape(-1, output_dim)\n            trg = trg[:, 1:].reshape(-1)\n\n            loss = criterion(output, trg)\n            total_loss += loss.item()\n\n    return total_loss / len(dataloader)","metadata":{"_uuid":"4a7fa058-fb98-4f9a-a410-0a4bea6299ba","_cell_guid":"df06c2e6-6edb-418b-aa11-3e78b904e28b","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.469345Z","iopub.execute_input":"2025-04-16T13:43:53.469584Z","iopub.status.idle":"2025-04-16T13:43:53.487054Z","shell.execute_reply.started":"2025-04-16T13:43:53.469564Z","shell.execute_reply":"2025-04-16T13:43:53.486495Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\nfrom nltk.tokenize import word_tokenize\n\n\ndef compute_bleu(model, data_loader, input_vocab, target_vocab):\n    model.eval()\n    total_score = 0\n    count = 0\n    smoothie = SmoothingFunction().method4\n\n    for src, trg in data_loader:\n        src, trg = src.to(device), trg.to(device)\n        for i in range(src.size(0)):\n            src_sentence = input_vocab.decode(src[i].tolist())\n            trg_sentence = target_vocab.decode(trg[i].tolist())\n\n            trg_sentence = [tok for tok in trg_sentence if tok not in ['<pad>', '<sos>', '<eos>']]\n            predicted = translate_sentence(model, ' '.join(src_sentence[1:-1]), input_vocab, target_vocab, device)\n            predicted_tokens = word_tokenize(predicted)\n\n            score = sentence_bleu(\n                [trg_sentence],\n                predicted_tokens,\n                smoothing_function=smoothie,\n                weights=(0.25, 0.25, 0.25, 0.25)\n            )\n            total_score += score\n            count += 1\n\n    return total_score / count if count > 0 else 0","metadata":{"_uuid":"f68f42de-2ef1-4ff2-a219-5db9e027036c","_cell_guid":"3baca943-44c2-4a26-9a06-c113da8fab0c","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.487708Z","iopub.execute_input":"2025-04-16T13:43:53.487949Z","iopub.status.idle":"2025-04-16T13:43:53.502942Z","shell.execute_reply.started":"2025-04-16T13:43:53.487923Z","shell.execute_reply":"2025-04-16T13:43:53.502375Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"def translate_sentence(model, sentence, input_vocab, target_vocab, device, max_len=50):\n    model.eval()\n    tokens = ['<sos>'] + preprocess_sentence(sentence) + ['<eos>']\n    numericalized = [input_vocab.token_to_idx.get(tok, input_vocab.token_to_idx['<unk>']) for tok in tokens]\n    tensor = torch.tensor(numericalized, dtype=torch.long).unsqueeze(0).to(device)\n\n    with torch.no_grad():\n        hidden, cell = model.encoder(tensor)\n\n    outputs = []\n    input_tok = torch.tensor([target_vocab.token_to_idx['<sos>']], dtype=torch.long).to(device)\n\n    for _ in range(max_len):\n        with torch.no_grad():\n            output, hidden, cell = model.decoder(input_tok, hidden, cell)\n            top1 = output.argmax(1)\n            outputs.append(top1.item())\n\n            if top1.item() == target_vocab.token_to_idx['<eos>']:\n                break\n            input_tok = top1\n\n    return ' '.join(target_vocab.decode(outputs))","metadata":{"_uuid":"8035beb1-0967-4245-91cf-cd4ac4663ca5","_cell_guid":"857fa276-7447-4c0d-a038-429360ed9760","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.503605Z","iopub.execute_input":"2025-04-16T13:43:53.503787Z","iopub.status.idle":"2025-04-16T13:43:53.522866Z","shell.execute_reply.started":"2025-04-16T13:43:53.503772Z","shell.execute_reply":"2025-04-16T13:43:53.522342Z"},"jupyter":{"outputs_hidden":false}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Load data with test split\ninp_train, inp_val, inp_test, targ_train, targ_val, targ_test, input_vocab, target_vocab = build_dataset(\n    '/kaggle/input/seq2seqdata/spa.txt', num_samples=10000\n)\n\n# DataLoaders\nBATCH_SIZE = 64\ntrain_dl = DataLoader(TensorDataset(inp_train, targ_train), batch_size=BATCH_SIZE, shuffle=True)\nval_dl = DataLoader(TensorDataset(inp_val, targ_val), batch_size=BATCH_SIZE)\ntest_dl = DataLoader(TensorDataset(inp_test, targ_test), batch_size=1)  # For BLEU, use batch_size=1\n\n# Model Setup\nINPUT_DIM = len(input_vocab)\nOUTPUT_DIM = len(target_vocab)\nENC_EMB_DIM = 256\nDEC_EMB_DIM = 256\nHID_DIM = 512\nN_LAYERS = 2\nDROPOUT = 0.5\n\nencoder = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\ndecoder = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DROPOUT)\nmodel = Seq2Seq(encoder, decoder, device).to(device)\n\noptimizer = torch.optim.Adam(model.parameters())\ncriterion = nn.CrossEntropyLoss(ignore_index=target_vocab.token_to_idx['<pad>'])\n\n# Train\nN_EPOCHS = 25\nfor epoch in range(N_EPOCHS):\n    train_loss = train_model(model, train_dl, optimizer, criterion, clip=1, device=device)\n    val_loss = evaluate_model(model, val_dl, criterion, device=device)\n    print(f\"Epoch {epoch+1}/{N_EPOCHS} | Train Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}\")\n\n# ‚úÖ Evaluate on the test set\nbleu = compute_bleu(model, test_dl, input_vocab, target_vocab)\nprint(f\"\\nüìè BLEU score on test set: {bleu:.4f}\")\n\n# Translate\nprint(\"\\n Sample Translations:\")\nsample_sentences = [\"Hello.\", \"I am fine.\", \"How are you?\", \"Where is the bathroom?\"]\nfor sentence in sample_sentences:\n    translation = translate_sentence(model, sentence, input_vocab, target_vocab, device)\n    print(f\"{sentence} ‚Üí {translation}\")","metadata":{"_uuid":"5fccbe05-2d1c-45cd-a3a9-a16077e35f12","_cell_guid":"3d868aa1-c9d0-43cc-8fd3-980e778bcfa9","trusted":true,"collapsed":false,"execution":{"iopub.status.busy":"2025-04-16T13:43:53.524529Z","iopub.execute_input":"2025-04-16T13:43:53.524755Z","iopub.status.idle":"2025-04-16T13:45:42.922057Z","shell.execute_reply.started":"2025-04-16T13:43:53.524741Z","shell.execute_reply":"2025-04-16T13:45:42.921377Z"},"jupyter":{"outputs_hidden":false}},"outputs":[{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:04<00:00, 27.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/25 | Train Loss: 4.616 | Val Loss: 4.731\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/25 | Train Loss: 3.774 | Val Loss: 4.419\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.40it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 3/25 | Train Loss: 3.309 | Val Loss: 4.251\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.73it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 4/25 | Train Loss: 2.956 | Val Loss: 4.075\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 5/25 | Train Loss: 2.605 | Val Loss: 4.062\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.33it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 6/25 | Train Loss: 2.318 | Val Loss: 4.086\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.90it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 7/25 | Train Loss: 2.055 | Val Loss: 4.021\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.55it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 8/25 | Train Loss: 1.807 | Val Loss: 4.040\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 9/25 | Train Loss: 1.598 | Val Loss: 4.100\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.53it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 10/25 | Train Loss: 1.385 | Val Loss: 4.136\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.36it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 11/25 | Train Loss: 1.188 | Val Loss: 4.184\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.37it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 12/25 | Train Loss: 1.035 | Val Loss: 4.233\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 13/25 | Train Loss: 0.913 | Val Loss: 4.215\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.65it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 14/25 | Train Loss: 0.784 | Val Loss: 4.289\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.30it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 15/25 | Train Loss: 0.690 | Val Loss: 4.364\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 16/25 | Train Loss: 0.632 | Val Loss: 4.348\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.35it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 17/25 | Train Loss: 0.571 | Val Loss: 4.400\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 18/25 | Train Loss: 0.522 | Val Loss: 4.493\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.16it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 19/25 | Train Loss: 0.506 | Val Loss: 4.540\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.34it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 20/25 | Train Loss: 0.476 | Val Loss: 4.509\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.64it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 21/25 | Train Loss: 0.466 | Val Loss: 4.536\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 32.10it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 22/25 | Train Loss: 0.440 | Val Loss: 4.580\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 23/25 | Train Loss: 0.443 | Val Loss: 4.589\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 24/25 | Train Loss: 0.432 | Val Loss: 4.573\n","output_type":"stream"},{"name":"stderr","text":"Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 125/125 [00:03<00:00, 31.69it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch 25/25 | Train Loss: 0.398 | Val Loss: 4.648\n\nüìè BLEU score on test set: 0.0807\n\n Sample Translations:\nHello. ‚Üí pagar . <eos>\nI am fine. ‚Üí soy muy carro . <eos>\nHow are you? ‚Üí ¬ø c mo est is ? <eos>\nWhere is the bathroom? ‚Üí ¬ø d nde est par s ? <eos>\n","output_type":"stream"}],"execution_count":7}]}