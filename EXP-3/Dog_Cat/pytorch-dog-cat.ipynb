{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":30378,"sourceType":"datasetVersion","datasetId":23777}],"dockerImageVersionId":30840,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F\nimport os\n\n# Define CNN Architecture\nclass CNN(nn.Module):\n    def __init__(self, activation='relu', init='xavier'):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.fc1 = nn.Linear(128 * 16 * 16, 256)  # Assuming input images are 128x128 after resizing\n        self.fc2 = nn.Linear(256, 2)  # 2 classes: Cat and Dog\n        self.dropout = nn.Dropout(0.5)\n        self.batch_norm = nn.BatchNorm2d(128)\n        \n        self.activation = self.get_activation(activation)\n        self.init_weights(init)\n\n    def forward(self, x):\n        x = self.pool(self.activation(self.conv1(x)))\n        x = self.pool(self.activation(self.conv2(x)))\n        x = self.pool(self.activation(self.conv3(x)))\n        x = self.batch_norm(x)\n        x = x.view(x.size(0), -1)\n        x = self.dropout(self.activation(self.fc1(x)))\n        x = self.fc2(x)\n        return x\n    \n    def get_activation(self, name):\n        if name == 'relu':\n            return F.relu\n        elif name == 'tanh':\n            return F.tanh\n        elif name == 'leaky_relu':\n            return F.leaky_relu\n        else:\n            raise ValueError(\"Unsupported activation function\")\n    \n    def init_weights(self, init_type):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n                if init_type == 'xavier':\n                    nn.init.xavier_uniform_(m.weight)\n                elif init_type == 'kaiming':\n                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n                elif init_type == 'random':\n                    nn.init.uniform_(m.weight, -0.1, 0.1)\n                if m.bias is not None:\n                    nn.init.zeros_(m.bias)\n\n# Prepare DataLoader for Cats vs. Dogs Dataset\ndef get_cats_dogs_dataloader(batch_size=64):\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    # Make sure the path is correct\n    train_set = datasets.ImageFolder(root='/kaggle/input/cat-and-dog/training_set/training_set', transform=transform)\n    test_set = datasets.ImageFolder(root='/kaggle/input/cat-and-dog/test_set/test_set', transform=transform)\n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n    return train_loader, test_loader\n\n# Training Function\ndef train_model(model, train_loader, optimizer, criterion, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n    return model\n\n# Evaluation Function\ndef evaluate_model(model, test_loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    return accuracy\n\n# Configurations for Optimizer and Loss Function\noptimizers = {'sgd': optim.SGD, 'adam': optim.Adam, 'rmsprop': optim.RMSprop}\ncriterion = nn.CrossEntropyLoss()\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Prepare data loaders for Cats vs. Dogs\ntrain_loader, test_loader = get_cats_dogs_dataloader()\n\n# Experimenting with different optimizers, activations, and initializations\nbest_accuracy = 0\nbest_model = None\n\n# Train the model with different configurations\nfor opt_name, opt_fn in optimizers.items():\n    for activation in ['relu', 'tanh', 'leaky_relu']:\n        for init in ['xavier', 'kaiming', 'random']:\n            print(f\"Training CNN with optimizer: {opt_name}, activation: {activation}, init: {init}\")\n            model = CNN(activation, init).to(device)\n            optimizer = opt_fn(model.parameters(), lr=0.001)\n            model = train_model(model, train_loader, optimizer, criterion, epochs=10)\n            acc = evaluate_model(model, test_loader)\n            if acc > best_accuracy:\n                best_accuracy = acc\n                best_model = model\n\n# Save the best model\ntorch.save(best_model.state_dict(), \"best_cnn_cats_dogs_model.pth\")\nprint(f\"Best CNN model saved with accuracy: {best_accuracy}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-02T17:29:53.388066Z","iopub.execute_input":"2025-02-02T17:29:53.388348Z","iopub.status.idle":"2025-02-02T19:31:12.591684Z","shell.execute_reply.started":"2025-02-02T17:29:53.388328Z","shell.execute_reply":"2025-02-02T19:31:12.590897Z"}},"outputs":[{"name":"stdout","text":"Training CNN with optimizer: sgd, activation: relu, init: xavier\nEpoch [1/10], Loss: 0.8642\nEpoch [2/10], Loss: 0.6485\nEpoch [3/10], Loss: 0.5654\nEpoch [4/10], Loss: 0.5190\nEpoch [5/10], Loss: 0.4829\nEpoch [6/10], Loss: 0.4508\nEpoch [7/10], Loss: 0.4231\nEpoch [8/10], Loss: 0.3996\nEpoch [9/10], Loss: 0.3802\nEpoch [10/10], Loss: 0.3524\nTest Accuracy: 72.17%\nTraining CNN with optimizer: sgd, activation: relu, init: kaiming\nEpoch [1/10], Loss: 0.9204\nEpoch [6/10], Loss: 0.4380\nEpoch [7/10], Loss: 0.4149\nEpoch [8/10], Loss: 0.3832\nEpoch [9/10], Loss: 0.3581\nEpoch [10/10], Loss: 0.3327\nTest Accuracy: 70.64%\nTraining CNN with optimizer: sgd, activation: relu, init: random\nEpoch [1/10], Loss: 2.8454\nEpoch [2/10], Loss: 1.1920\nEpoch [3/10], Loss: 0.7943\nEpoch [4/10], Loss: 0.7154\nEpoch [5/10], Loss: 0.6946\nEpoch [6/10], Loss: 0.6777\nEpoch [7/10], Loss: 0.6691\nEpoch [8/10], Loss: 0.6753\nEpoch [9/10], Loss: 0.6508\nEpoch [10/10], Loss: 0.6595\nTest Accuracy: 63.57%\nTraining CNN with optimizer: sgd, activation: tanh, init: xavier\nEpoch [1/10], Loss: 0.8679\nEpoch [2/10], Loss: 0.7465\nEpoch [3/10], Loss: 0.6891\nEpoch [4/10], Loss: 0.6308\nEpoch [5/10], Loss: 0.5993\nEpoch [6/10], Loss: 0.5690\nEpoch [7/10], Loss: 0.5278\nEpoch [8/10], Loss: 0.5044\nEpoch [9/10], Loss: 0.4804\nEpoch [10/10], Loss: 0.4605\nTest Accuracy: 70.39%\nTraining CNN with optimizer: sgd, activation: tanh, init: kaiming\nEpoch [1/10], Loss: 0.9038\nEpoch [2/10], Loss: 0.7752\nEpoch [3/10], Loss: 0.7078\nEpoch [4/10], Loss: 0.6413\nEpoch [5/10], Loss: 0.6095\nEpoch [6/10], Loss: 0.5550\nEpoch [7/10], Loss: 0.5341\nEpoch [8/10], Loss: 0.5015\nEpoch [9/10], Loss: 0.4757\nEpoch [10/10], Loss: 0.4532\nTest Accuracy: 71.08%\nTraining CNN with optimizer: sgd, activation: tanh, init: random\nEpoch [1/10], Loss: 0.9153\nEpoch [2/10], Loss: 0.8434\nEpoch [3/10], Loss: 0.7961\nEpoch [4/10], Loss: 0.7683\nEpoch [5/10], Loss: 0.7391\nEpoch [6/10], Loss: 0.7211\nEpoch [7/10], Loss: 0.6996\nEpoch [8/10], Loss: 0.6903\nEpoch [9/10], Loss: 0.6805\nEpoch [10/10], Loss: 0.6679\nTest Accuracy: 62.88%\nTraining CNN with optimizer: sgd, activation: leaky_relu, init: xavier\nEpoch [1/10], Loss: 0.8562\nEpoch [2/10], Loss: 0.6179\nEpoch [3/10], Loss: 0.5605\nEpoch [4/10], Loss: 0.5153\nEpoch [5/10], Loss: 0.4803\nEpoch [6/10], Loss: 0.4439\nEpoch [7/10], Loss: 0.4198\nEpoch [8/10], Loss: 0.3953\nEpoch [9/10], Loss: 0.3845\nEpoch [10/10], Loss: 0.3633\nTest Accuracy: 70.93%\nTraining CNN with optimizer: sgd, activation: leaky_relu, init: kaiming\nEpoch [1/10], Loss: 0.8684\nEpoch [2/10], Loss: 0.6591\nEpoch [3/10], Loss: 0.5737\nEpoch [4/10], Loss: 0.5296\nEpoch [5/10], Loss: 0.4910\nEpoch [6/10], Loss: 0.4653\nEpoch [7/10], Loss: 0.4299\nEpoch [8/10], Loss: 0.4117\nEpoch [9/10], Loss: 0.3900\nEpoch [10/10], Loss: 0.3715\nTest Accuracy: 71.08%\nTraining CNN with optimizer: sgd, activation: leaky_relu, init: random\nEpoch [1/10], Loss: 2.8177\nEpoch [2/10], Loss: 1.1921\nEpoch [3/10], Loss: 0.7871\nEpoch [4/10], Loss: 0.7148\nEpoch [5/10], Loss: 0.6941\nEpoch [6/10], Loss: 0.6766\nEpoch [7/10], Loss: 0.6611\nEpoch [8/10], Loss: 0.6631\nEpoch [9/10], Loss: 0.6578\nEpoch [10/10], Loss: 0.6615\nTest Accuracy: 50.77%\nTraining CNN with optimizer: adam, activation: relu, init: xavier\nEpoch [1/10], Loss: 1.0044\nEpoch [2/10], Loss: 0.5869\nEpoch [3/10], Loss: 0.5352\nEpoch [4/10], Loss: 0.4668\nEpoch [5/10], Loss: 0.4073\nEpoch [6/10], Loss: 0.3607\nEpoch [7/10], Loss: 0.3006\nEpoch [8/10], Loss: 0.2552\nEpoch [9/10], Loss: 0.2279\nEpoch [10/10], Loss: 0.1562\nTest Accuracy: 78.10%\nTraining CNN with optimizer: adam, activation: relu, init: kaiming\nEpoch [1/10], Loss: 1.1544\nEpoch [2/10], Loss: 0.5879\nEpoch [3/10], Loss: 0.5183\nEpoch [4/10], Loss: 0.4662\nEpoch [5/10], Loss: 0.4017\nEpoch [6/10], Loss: 0.3478\nEpoch [7/10], Loss: 0.2674\nEpoch [8/10], Loss: 0.2230\nEpoch [9/10], Loss: 0.1915\nEpoch [10/10], Loss: 0.1350\nTest Accuracy: 74.84%\nTraining CNN with optimizer: adam, activation: relu, init: random\nEpoch [1/10], Loss: 1.7355\nEpoch [2/10], Loss: 0.6019\nEpoch [3/10], Loss: 0.5654\nEpoch [4/10], Loss: 0.5377\nEpoch [5/10], Loss: 0.5009\nEpoch [6/10], Loss: 0.4513\nEpoch [7/10], Loss: 0.3842\nEpoch [8/10], Loss: 0.3219\nEpoch [9/10], Loss: 0.2935\nEpoch [10/10], Loss: 0.2377\nTest Accuracy: 74.39%\nTraining CNN with optimizer: adam, activation: tanh, init: xavier\nEpoch [1/10], Loss: 1.0539\nEpoch [2/10], Loss: 0.7725\nEpoch [3/10], Loss: 0.6844\nEpoch [4/10], Loss: 0.6399\nEpoch [5/10], Loss: 0.6328\nEpoch [6/10], Loss: 0.6077\nEpoch [7/10], Loss: 0.6046\nEpoch [8/10], Loss: 0.6015\nEpoch [9/10], Loss: 0.6033\nEpoch [10/10], Loss: 0.5921\nTest Accuracy: 69.01%\nTraining CNN with optimizer: adam, activation: tanh, init: kaiming\nEpoch [1/10], Loss: 1.1039\nEpoch [2/10], Loss: 0.7667\nEpoch [3/10], Loss: 0.6555\nEpoch [4/10], Loss: 0.6330\nEpoch [5/10], Loss: 0.6039\nEpoch [6/10], Loss: 0.5830\nEpoch [7/10], Loss: 0.5804\nEpoch [8/10], Loss: 0.5549\nEpoch [9/10], Loss: 0.5447\nEpoch [10/10], Loss: 0.5409\nTest Accuracy: 71.87%\nTraining CNN with optimizer: adam, activation: tanh, init: random\nEpoch [1/10], Loss: 0.7951\nEpoch [2/10], Loss: 0.6743\nEpoch [3/10], Loss: 0.6473\nEpoch [4/10], Loss: 0.6339\nEpoch [5/10], Loss: 0.6183\nEpoch [6/10], Loss: 0.6148\nEpoch [7/10], Loss: 0.6054\nEpoch [8/10], Loss: 0.6090\nEpoch [9/10], Loss: 0.6021\nEpoch [10/10], Loss: 0.5903\nTest Accuracy: 68.61%\nTraining CNN with optimizer: adam, activation: leaky_relu, init: xavier\nEpoch [1/10], Loss: 0.9537\nEpoch [2/10], Loss: 0.6024\nEpoch [3/10], Loss: 0.5523\nEpoch [4/10], Loss: 0.5149\nEpoch [5/10], Loss: 0.4744\nEpoch [6/10], Loss: 0.4439\nEpoch [7/10], Loss: 0.3666\nEpoch [8/10], Loss: 0.3094\nEpoch [9/10], Loss: 0.2545\nEpoch [10/10], Loss: 0.2132\nTest Accuracy: 77.90%\nTraining CNN with optimizer: adam, activation: leaky_relu, init: kaiming\nEpoch [1/10], Loss: 1.0360\nEpoch [2/10], Loss: 0.5777\nEpoch [3/10], Loss: 0.5039\nEpoch [4/10], Loss: 0.4471\nEpoch [5/10], Loss: 0.3782\nEpoch [6/10], Loss: 0.3145\nEpoch [7/10], Loss: 0.2474\nEpoch [8/10], Loss: 0.2123\nEpoch [9/10], Loss: 0.1708\nEpoch [10/10], Loss: 0.1491\nTest Accuracy: 77.16%\nTraining CNN with optimizer: adam, activation: leaky_relu, init: random\nEpoch [1/10], Loss: 1.8283\nEpoch [2/10], Loss: 0.6072\nEpoch [3/10], Loss: 0.5675\nEpoch [4/10], Loss: 0.5353\nEpoch [5/10], Loss: 0.4950\nEpoch [6/10], Loss: 0.4497\nEpoch [7/10], Loss: 0.3860\nEpoch [8/10], Loss: 0.3251\nEpoch [9/10], Loss: 0.2870\nEpoch [10/10], Loss: 0.2695\nTest Accuracy: 77.85%\nTraining CNN with optimizer: rmsprop, activation: relu, init: xavier\nEpoch [1/10], Loss: 2.8810\nEpoch [2/10], Loss: 0.6390\nEpoch [3/10], Loss: 0.6083\nEpoch [4/10], Loss: 0.5782\nEpoch [5/10], Loss: 0.5449\nEpoch [6/10], Loss: 0.5080\nEpoch [7/10], Loss: 0.4726\nEpoch [8/10], Loss: 0.4214\nEpoch [9/10], Loss: 0.3645\nEpoch [10/10], Loss: 0.3161\nTest Accuracy: 63.96%\nTraining CNN with optimizer: rmsprop, activation: relu, init: kaiming\nEpoch [1/10], Loss: 2.5235\nEpoch [2/10], Loss: 0.6240\nEpoch [3/10], Loss: 0.5841\nEpoch [4/10], Loss: 0.5586\nEpoch [5/10], Loss: 0.5433\nEpoch [6/10], Loss: 0.4791\nEpoch [7/10], Loss: 0.4331\nEpoch [8/10], Loss: 0.3739\nEpoch [9/10], Loss: 0.3289\nEpoch [10/10], Loss: 0.2739\nTest Accuracy: 76.57%\nTraining CNN with optimizer: rmsprop, activation: relu, init: random\nEpoch [1/10], Loss: 1.8614\nEpoch [2/10], Loss: 0.6409\nEpoch [3/10], Loss: 0.6151\nEpoch [4/10], Loss: 0.5992\nEpoch [5/10], Loss: 0.5457\nEpoch [6/10], Loss: 0.5099\nEpoch [7/10], Loss: 0.4626\nEpoch [8/10], Loss: 0.4093\nEpoch [9/10], Loss: 0.3414\nEpoch [10/10], Loss: 0.2961\nTest Accuracy: 72.71%\nTraining CNN with optimizer: rmsprop, activation: tanh, init: xavier\nEpoch [1/10], Loss: 0.9031\nEpoch [2/10], Loss: 0.7130\nEpoch [3/10], Loss: 0.6623\nEpoch [4/10], Loss: 0.6525\nEpoch [5/10], Loss: 0.6428\nEpoch [6/10], Loss: 0.6311\nEpoch [7/10], Loss: 0.6219\nEpoch [8/10], Loss: 0.6267\nEpoch [9/10], Loss: 0.6213\nEpoch [10/10], Loss: 0.5979\nTest Accuracy: 67.23%\nTraining CNN with optimizer: rmsprop, activation: tanh, init: kaiming\nEpoch [1/10], Loss: 0.9593\nEpoch [2/10], Loss: 0.7045\nEpoch [3/10], Loss: 0.6632\nEpoch [4/10], Loss: 0.6649\nEpoch [5/10], Loss: 0.6421\nEpoch [6/10], Loss: 0.6316\nEpoch [7/10], Loss: 0.6173\nEpoch [8/10], Loss: 0.6011\nEpoch [9/10], Loss: 0.5989\nEpoch [10/10], Loss: 0.6031\nTest Accuracy: 64.95%\nTraining CNN with optimizer: rmsprop, activation: tanh, init: random\nEpoch [1/10], Loss: 0.8004\nEpoch [2/10], Loss: 0.6871\nEpoch [3/10], Loss: 0.6595\nEpoch [4/10], Loss: 0.6566\nEpoch [5/10], Loss: 0.6501\nEpoch [6/10], Loss: 0.6410\nEpoch [7/10], Loss: 0.6424\nEpoch [8/10], Loss: 0.6561\nEpoch [9/10], Loss: 0.6350\nEpoch [10/10], Loss: 0.6292\nTest Accuracy: 66.24%\nTraining CNN with optimizer: rmsprop, activation: leaky_relu, init: xavier\nEpoch [1/10], Loss: 3.2413\nEpoch [2/10], Loss: 0.7014\nEpoch [3/10], Loss: 0.7137\nEpoch [4/10], Loss: 0.7305\nEpoch [5/10], Loss: 0.6563\nEpoch [6/10], Loss: 0.6235\nEpoch [7/10], Loss: 0.5430\nEpoch [8/10], Loss: 0.5237\nEpoch [9/10], Loss: 0.4606\nEpoch [10/10], Loss: 0.3874\nTest Accuracy: 51.41%\nTraining CNN with optimizer: rmsprop, activation: leaky_relu, init: kaiming\nEpoch [1/10], Loss: 3.8255\nEpoch [2/10], Loss: 0.6603\nEpoch [3/10], Loss: 0.6505\nEpoch [4/10], Loss: 0.6410\nEpoch [5/10], Loss: 0.6098\nEpoch [6/10], Loss: 0.5488\nEpoch [7/10], Loss: 0.4970\nEpoch [8/10], Loss: 0.4386\nEpoch [9/10], Loss: 0.3801\nEpoch [10/10], Loss: 0.3241\nTest Accuracy: 65.55%\nTraining CNN with optimizer: rmsprop, activation: leaky_relu, init: random\nEpoch [1/10], Loss: 2.0683\nEpoch [2/10], Loss: 0.6522\nEpoch [3/10], Loss: 0.6385\nEpoch [4/10], Loss: 0.6188\nEpoch [5/10], Loss: 0.5890\nEpoch [6/10], Loss: 0.5530\nEpoch [7/10], Loss: 0.5111\nEpoch [8/10], Loss: 0.4311\nEpoch [9/10], Loss: 0.3833\nEpoch [10/10], Loss: 0.3309\nTest Accuracy: 78.05%\nBest CNN model saved with accuracy: 78.10182896688087\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# **Fine-Tune Resnet-18**","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nfrom torch.utils.data import DataLoader\nimport os\nfrom torchvision import models\n\n# Device configuration\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Fine-Tune ResNet-18 for CIFAR-10 with Extra Layers\ndef fine_tune_resnet18():\n    model = models.resnet18(pretrained=True)\n\n    # Unfreeze the last 2 blocks for fine-tuning\n    for param in model.parameters():\n        param.requires_grad = False  # Freeze all layers\n\n    for param in list(model.layer4.parameters()):  # Unfreeze the last block\n        param.requires_grad = True\n\n    for param in list(model.layer3.parameters()):  # Unfreeze the second last block\n        param.requires_grad = True\n\n    num_ftrs = model.fc.in_features\n\n    # Modify the classifier with extra layers\n    model.fc = nn.Sequential(\n        nn.Linear(num_ftrs, 1024),  \n        nn.LeakyReLU(negative_slope=0.01),\n        nn.BatchNorm1d(1024),\n        nn.Linear(1024, 512),\n        nn.LeakyReLU(negative_slope=0.01),\n        nn.BatchNorm1d(512),\n        nn.Dropout(0.3),\n        nn.Linear(512, 2)  # Output layer for Cats vs. Dogs (2 classes)\n    )\n    \n    return model.to(device)\n\n# Prepare DataLoader for Cats vs. Dogs Dataset\ndef get_cats_dogs_dataloader(batch_size=64):\n    transform = transforms.Compose([\n        transforms.Resize((128, 128)),\n        transforms.ToTensor(),\n        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n    ])\n    \n    train_set = datasets.ImageFolder(root='/kaggle/input/cat-and-dog/training_set/training_set', transform=transform)\n    test_set = datasets.ImageFolder(root='/kaggle/input/cat-and-dog/test_set/test_set', transform=transform)\n    \n    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n    \n    return train_loader, test_loader\n\n# Training Function\ndef train_model(model, train_loader, optimizer, criterion, epochs=10):\n    model.train()\n    for epoch in range(epochs):\n        running_loss = 0.0\n        for inputs, labels in train_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n    return model\n\n# Evaluation Function\ndef evaluate_model(model, test_loader):\n    model.eval()\n    correct, total = 0, 0\n    with torch.no_grad():\n        for inputs, labels in test_loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    accuracy = 100 * correct / total\n    print(f'Test Accuracy: {accuracy:.2f}%')\n    return accuracy\n\n# Load Data\ntrain_loader, test_loader = get_cats_dogs_dataloader(batch_size=64)\n\n# Configurations for Optimizer and Loss Function\noptimizers = {'sgd': optim.SGD, 'adam': optim.Adam, 'rmsprop': optim.RMSprop}\ncriterion = nn.CrossEntropyLoss()\n\n# Fine-tune ResNet-18\nbest_accuracy = 0\nbest_model = None\n\nfor opt_name, opt_fn in optimizers.items():\n    print(f\"Training ResNet-18 with optimizer: {opt_name}\")\n    model = fine_tune_resnet18()\n    optimizer = opt_fn(model.parameters(), lr=0.001)\n    model = train_model(model, train_loader, optimizer, criterion, epochs=10)\n    acc = evaluate_model(model, test_loader)\n    \n    if acc > best_accuracy:\n        best_accuracy = acc\n        best_model = model\n\n# Save the Best Model\ntorch.save(best_model.state_dict(), \"best_resnet18_model.pth\")\nprint(\"Best model saved with accuracy:\", best_accuracy)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-03T13:07:27.729036Z","iopub.execute_input":"2025-02-03T13:07:27.729237Z","iopub.status.idle":"2025-02-03T13:24:36.140502Z","shell.execute_reply.started":"2025-02-03T13:07:27.729215Z","shell.execute_reply":"2025-02-03T13:24:36.139741Z"}},"outputs":[{"name":"stdout","text":"Training ResNet-18 with optimizer: sgd\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n  warnings.warn(\n/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n  warnings.warn(msg)\nDownloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n100%|██████████| 44.7M/44.7M [00:00<00:00, 185MB/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 0.4441\nEpoch [2/10], Loss: 0.2698\nEpoch [3/10], Loss: 0.2135\nEpoch [4/10], Loss: 0.1816\nEpoch [5/10], Loss: 0.1602\nEpoch [6/10], Loss: 0.1597\nEpoch [7/10], Loss: 0.1305\nEpoch [8/10], Loss: 0.1268\nEpoch [9/10], Loss: 0.1165\nEpoch [10/10], Loss: 0.1044\nTest Accuracy: 94.17%\nTraining ResNet-18 with optimizer: adam\nEpoch [1/10], Loss: 0.1930\nEpoch [2/10], Loss: 0.1214\nEpoch [3/10], Loss: 0.0623\nEpoch [4/10], Loss: 0.0409\nEpoch [5/10], Loss: 0.0537\nEpoch [6/10], Loss: 0.0603\nEpoch [7/10], Loss: 0.0824\nEpoch [8/10], Loss: 0.0287\nEpoch [9/10], Loss: 0.0282\nEpoch [10/10], Loss: 0.0345\nTest Accuracy: 95.35%\nTraining ResNet-18 with optimizer: rmsprop\nEpoch [1/10], Loss: 0.2603\nEpoch [2/10], Loss: 0.1304\nEpoch [3/10], Loss: 0.0876\nEpoch [4/10], Loss: 0.0630\nEpoch [5/10], Loss: 0.0823\nEpoch [6/10], Loss: 0.0421\nEpoch [7/10], Loss: 0.0320\nEpoch [8/10], Loss: 0.0317\nEpoch [9/10], Loss: 0.0637\nEpoch [10/10], Loss: 0.0290\nTest Accuracy: 93.13%\nBest model saved with accuracy: 95.3534354918438\n","output_type":"stream"}],"execution_count":1}]}